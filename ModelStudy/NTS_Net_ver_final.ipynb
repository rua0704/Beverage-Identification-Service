{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "LdKDatnsQJb5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32956,
     "status": "ok",
     "timestamp": 1630036860131,
     "user": {
      "displayName": "Gyeom",
      "photoUrl": "",
      "userId": "17196245438344588530"
     },
     "user_tz": -540
    },
    "id": "LdKDatnsQJb5",
    "outputId": "8703ad16-d56b-44b8-be8d-81e3cf968631"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70277fa2",
   "metadata": {
    "executionInfo": {
     "elapsed": 4151,
     "status": "ok",
     "timestamp": 1630036864272,
     "user": {
      "displayName": "Gyeom",
      "photoUrl": "",
      "userId": "17196245438344588530"
     },
     "user_tz": -540
    },
    "id": "70277fa2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.utils.data\n",
    "from torch.nn import DataParallel\n",
    "from datetime import datetime\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "115ef8d1",
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1630036864273,
     "user": {
      "displayName": "Gyeom",
      "photoUrl": "",
      "userId": "17196245438344588530"
     },
     "user_tz": -540
    },
    "id": "115ef8d1"
   },
   "outputs": [],
   "source": [
    "#parameter 수정 가능\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "PROPOSAL_NUM = 6\n",
    "CAT_NUM = 4\n",
    "INPUT_SIZE = (224, 224) \n",
    "LR = 0.001 \n",
    "WD = 1e-4\n",
    "SAVE_FREQ = 1\n",
    "resume = ''\n",
    "test_model = 'model.ckpt'\n",
    "save_dir = '/content/drive/MyDrive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d4c482",
   "metadata": {
    "executionInfo": {
     "elapsed": 42534,
     "status": "ok",
     "timestamp": 1630037075688,
     "user": {
      "displayName": "Gyeom",
      "photoUrl": "",
      "userId": "17196245438344588530"
     },
     "user_tz": -540
    },
    "id": "87d4c482"
   },
   "outputs": [],
   "source": [
    "# 데이터셋을 불러올 때 사용할 변형(transformation) 객체 정의\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 정규화(normalization)\n",
    "])\n",
    "\n",
    "transforms_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_dir = '/content/drive/MyDrive/data_100'\n",
    "all_sets = datasets.ImageFolder(os.path.join(data_dir, 'img100_rot'), transforms_train)  \n",
    "train_size = int(0.8 * len(all_sets))\n",
    "test_size = len(all_sets) - train_size\n",
    "\n",
    "train_datasets, test_datasets = torch.utils.data.random_split(all_sets, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "test_datasets, val_datasets = torch.utils.data.random_split(test_datasets, [int(test_size * 0.5), test_size - int(test_size * 0.5)], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(all_sets, batch_size=BATCH_SIZE , shuffle=True, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=BATCH_SIZE , shuffle=True, num_workers=2)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_datasets, batch_size=BATCH_SIZE , shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "007bd487",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1630037080639,
     "user": {
      "displayName": "Gyeom",
      "photoUrl": "",
      "userId": "17196245438344588530"
     },
     "user_tz": -540
    },
    "id": "007bd487",
    "outputId": "af285594-0b01-4c68-cf33-070db7546cb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 수: 105\n"
     ]
    }
   ],
   "source": [
    "print('학습 데이터셋 크기:', len(train_datasets)) #학습 데이터셋 크기: 16589\n",
    "print('val 데이터셋 크기:', len(val_datasets)) #val 데이터셋 크기: 2074\n",
    "print('테스트 데이터셋 크기:', len(test_datasets)) #테스트 데이터셋 크기: 2074\n",
    "\n",
    "class_names = all_sets.classes\n",
    "print('클래스 수:', len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1387485c",
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1630037090178,
     "user": {
      "displayName": "Gyeom",
      "photoUrl": "",
      "userId": "17196245438344588530"
     },
     "user_tz": -540
    },
    "id": "1387485c"
   },
   "outputs": [],
   "source": [
    "# utils\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "term_width = 80 \n",
    "term_width = int(term_width)\n",
    "\n",
    "TOTAL_BAR_LENGTH = 40.\n",
    "last_time = time.time()\n",
    "begin_time = last_time\n",
    "\n",
    "\n",
    "def progress_bar(current, total, msg=None):\n",
    "    global last_time, begin_time\n",
    "    if current == 0:\n",
    "        begin_time = time.time()  # Reset for new bar.\n",
    "\n",
    "    cur_len = int(TOTAL_BAR_LENGTH * current / total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
    "\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(cur_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    cur_time = time.time()\n",
    "    step_time = cur_time - last_time\n",
    "    last_time = cur_time\n",
    "    tot_time = cur_time - begin_time\n",
    "\n",
    "    L = []\n",
    "    L.append('  Step: %s' % format_time(step_time))\n",
    "    L.append(' | Tot: %s' % format_time(tot_time))\n",
    "    if msg:\n",
    "        L.append(' | ' + msg)\n",
    "\n",
    "    msg = ''.join(L)\n",
    "    sys.stdout.write(msg)\n",
    "    for i in range(term_width - int(TOTAL_BAR_LENGTH) - len(msg) - 3):\n",
    "        sys.stdout.write(' ')\n",
    "\n",
    "    # Go back to the center of the bar.\n",
    "    for i in range(term_width - int(TOTAL_BAR_LENGTH / 2)):\n",
    "        sys.stdout.write('\\b')\n",
    "    sys.stdout.write(' %d/%d ' % (current + 1, total))\n",
    "\n",
    "    if current < total - 1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600 / 24)\n",
    "    seconds = seconds - days * 3600 * 24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours * 3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes * 60\n",
    "    secondsf = int(seconds)\n",
    "    seconds = seconds - secondsf\n",
    "    millis = int(seconds * 1000)\n",
    "\n",
    "    f = ''\n",
    "    i = 1\n",
    "    if days > 0:\n",
    "        f += str(days) + 'D'\n",
    "        i += 1\n",
    "    if hours > 0 and i <= 2:\n",
    "        f += str(hours) + 'h'\n",
    "        i += 1\n",
    "    if minutes > 0 and i <= 2:\n",
    "        f += str(minutes) + 'm'\n",
    "        i += 1\n",
    "    if secondsf > 0 and i <= 2:\n",
    "        f += str(secondsf) + 's'\n",
    "        i += 1\n",
    "    if millis > 0 and i <= 2:\n",
    "        f += str(millis) + 'ms'\n",
    "        i += 1\n",
    "    if f == '':\n",
    "        f = '0ms'\n",
    "    return f\n",
    "\n",
    "\n",
    "def init_log(output_dir):\n",
    "    logging.basicConfig(level=logging.DEBUG,\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        datefmt='%Y%m%d-%H:%M:%S',\n",
    "                        filename=os.path.join(output_dir, 'log.log'),\n",
    "                        filemode='w')\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.INFO)\n",
    "    logging.getLogger('').addHandler(console)\n",
    "    return logging\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4130e1f2",
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1630037098861,
     "user": {
      "displayName": "Gyeom",
      "photoUrl": "",
      "userId": "17196245438344588530"
     },
     "user_tz": -540
    },
    "id": "4130e1f2"
   },
   "outputs": [],
   "source": [
    "# anchor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "_default_anchors_setting = (# 3 parts\n",
    "    dict(layer='p3', stride=32, size=48, scale=[2 ** (1. / 3.), 2 ** (2. / 3.)], aspect_ratio=[0.667, 1, 1.5]),\n",
    "    dict(layer='p4', stride=64, size=96, scale=[2 ** (1. / 3.), 2 ** (2. / 3.)], aspect_ratio=[0.667, 1, 1.5]),\n",
    "    dict(layer='p5', stride=128, size=192, scale=[1, 2 ** (1. / 3.), 2 ** (2. / 3.)], aspect_ratio=[0.667, 1, 1.5]),\n",
    ")\n",
    "\n",
    "\n",
    "def generate_default_anchor_maps(anchors_setting=None, input_shape=INPUT_SIZE):\n",
    "    \"\"\"\n",
    "    generate default anchor\n",
    "    :param anchors_setting: all informations of anchors\n",
    "    :param input_shape: shape of input images, e.g. (h, w)\n",
    "    :return: center_anchors: # anchors * 4 (oy, ox, h, w)\n",
    "             edge_anchors: # anchors * 4 (y0, x0, y1, x1)\n",
    "             anchor_area: # anchors * 1 (area)\n",
    "    \"\"\"\n",
    "    if anchors_setting is None:\n",
    "        anchors_setting = _default_anchors_setting\n",
    "\n",
    "    center_anchors = np.zeros((0, 4), dtype=np.float32)\n",
    "    edge_anchors = np.zeros((0, 4), dtype=np.float32)\n",
    "    anchor_areas = np.zeros((0,), dtype=np.float32)\n",
    "    input_shape = np.array(input_shape, dtype=int)\n",
    "\n",
    "    for anchor_info in anchors_setting:\n",
    "\n",
    "        stride = anchor_info['stride']\n",
    "        size = anchor_info['size']\n",
    "        scales = anchor_info['scale']\n",
    "        aspect_ratios = anchor_info['aspect_ratio']\n",
    "\n",
    "        output_map_shape = np.ceil(input_shape.astype(np.float32) / stride) \n",
    "        output_map_shape = output_map_shape.astype(np.int)\n",
    "        output_shape = tuple(output_map_shape) + (4,)\n",
    "        ostart = stride / 2.\n",
    "        oy = np.arange(ostart, ostart + stride * output_shape[0], stride)\n",
    "        oy = oy.reshape(output_shape[0], 1)\n",
    "        ox = np.arange(ostart, ostart + stride * output_shape[1], stride)\n",
    "        ox = ox.reshape(1, output_shape[1])\n",
    "        center_anchor_map_template = np.zeros(output_shape, dtype=np.float32)\n",
    "        center_anchor_map_template[:, :, 0] = oy\n",
    "        center_anchor_map_template[:, :, 1] = ox\n",
    "        for scale in scales:\n",
    "            for aspect_ratio in aspect_ratios:\n",
    "                center_anchor_map = center_anchor_map_template.copy()\n",
    "                center_anchor_map[:, :, 2] = size * scale / float(aspect_ratio) ** 0.5\n",
    "                center_anchor_map[:, :, 3] = size * scale * float(aspect_ratio) ** 0.5\n",
    "\n",
    "                edge_anchor_map = np.concatenate((center_anchor_map[..., :2] - center_anchor_map[..., 2:4] / 2.,\n",
    "                                                  center_anchor_map[..., :2] + center_anchor_map[..., 2:4] / 2.),\n",
    "                                                 axis=-1)\n",
    "                anchor_area_map = center_anchor_map[..., 2] * center_anchor_map[..., 3]\n",
    "                center_anchors = np.concatenate((center_anchors, center_anchor_map.reshape(-1, 4)))\n",
    "                edge_anchors = np.concatenate((edge_anchors, edge_anchor_map.reshape(-1, 4)))\n",
    "                anchor_areas = np.concatenate((anchor_areas, anchor_area_map.reshape(-1)))\n",
    "\n",
    "    return center_anchors, edge_anchors, anchor_areas\n",
    "\n",
    "# predicted bounding box가 ground-truth bounding box와 얼마나 일치하는지 측정하기 위한 평가 지표를 정의\n",
    "# area of overlap / area of union\n",
    "def hard_nms(cdds, topn=10, iou_thresh=0.25): \n",
    "    #object detector가 예측한 bounding box 중에서 정확한 bounding box를 선택하도록 하는 기법\n",
    "    #iou_thresh:한계점\n",
    "    if not (type(cdds).__module__ == 'numpy' and len(cdds.shape) == 2 and cdds.shape[1] >= 5):\n",
    "        raise TypeError('edge_box_map should be N * 5+ ndarray')\n",
    "\n",
    "    cdds = cdds.copy()\n",
    "    indices = np.argsort(cdds[:, 0])\n",
    "    cdds = cdds[indices]\n",
    "    cdd_results = []\n",
    "\n",
    "    res = cdds\n",
    "\n",
    "    while res.any():\n",
    "        cdd = res[-1]\n",
    "        cdd_results.append(cdd)\n",
    "        if len(cdd_results) == topn:\n",
    "            return np.array(cdd_results)\n",
    "        res = res[:-1] \n",
    "\n",
    "        start_max = np.maximum(res[:, 1:3], cdd[1:3])# 두 개의 array에 대해 동일한 위치의 성분끼리 비교하여 최대값 또는 최소값 계산\n",
    "        end_min = np.minimum(res[:, 3:5], cdd[3:5])\n",
    "        lengths = end_min - start_max\n",
    "        intersec_map = lengths[:, 0] * lengths[:, 1]\n",
    "        intersec_map[np.logical_or(lengths[:, 0] < 0, lengths[:, 1] < 0)] = 0\n",
    "        iou_map_cur = intersec_map / ((res[:, 3] - res[:, 1]) * (res[:, 4] - res[:, 2]) + (cdd[3] - cdd[1]) * (\n",
    "            cdd[4] - cdd[2]) - intersec_map)\n",
    "        res = res[iou_map_cur < iou_thresh]\n",
    "\n",
    "    return np.array(cdd_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b00bd38",
   "metadata": {
    "executionInfo": {
     "elapsed": 664,
     "status": "ok",
     "timestamp": 1630037125910,
     "user": {
      "displayName": "Gyeom",
      "photoUrl": "",
      "userId": "17196245438344588530"
     },
     "user_tz": -540
    },
    "id": "8b00bd38"
   },
   "outputs": [],
   "source": [
    "# resnet\n",
    "# Batch normalization 사용\n",
    "\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample# downsampling:차원을 줄여서 적은 메모리로 깊은 convolution을 할 수 있게 한다. \n",
    "        # 보통 stride를 2 이상으로 하는 convolution을 사용하거나 pooling을 사용한다. \n",
    "        # 이 과정을 거치면 어쩔 수 없이 feature의 정보를 잃게된다.\n",
    "        # https://m.blog.naver.com/9709193/221979612209\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    # 1x1-> 3x3 -> 1x1\n",
    "    \n",
    "    # Convolution Parameters = Kernel Size x Kernel Size x Input Channel x Output Channel\n",
    "    #1x1 Convolution: 연산량이 작기 때문에 Feature Map(Output Channel)을 줄이거나 키울 때 사용\n",
    "    #차원, 채널 축소 후 공간적 특성 추출(3x3 Convolution이 연산량이 9배 많기 때문에), 채널 증가, parameters 감소\n",
    "\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)# If True, adds a learnable bias to the output.\n",
    "        ## BatchNorm에 bias가 포함되어 있으므로, conv2d는 bias=False로 설정합니다.\n",
    "        ## https://deep-learning-study.tistory.com/534\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)# num_features – C dimension from an expected input of size (N, C, H, W)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) #https://guru.tistory.com/70 예시:(1,1,64,64)->(1,1,32,32)\n",
    "                                                                            #maxpool2d(2)일 때\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels#################\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        feature1 = x\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.Dropout(p=0.5)(x)\n",
    "        feature2 = x\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x, feature1, feature2\n",
    "\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39579ffe",
   "metadata": {
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1630037130728,
     "user": {
      "displayName": "Gyeom",
      "photoUrl": "",
      "userId": "17196245438344588530"
     },
     "user_tz": -540
    },
    "id": "39579ffe"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ProposalNet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(ProposalNet, self).__init__()\n",
    "        self.down1 = nn.Conv2d(2048, 128, 3, 1, 1)\n",
    "        self.down2 = nn.Conv2d(128, 128, 3, 2, 1)\n",
    "        self.down3 = nn.Conv2d(128, 128, 3, 2, 1)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.tidy1 = nn.Conv2d(128, 6, 1, 1, 0)\n",
    "        self.tidy2 = nn.Conv2d(128, 6, 1, 1, 0)\n",
    "        self.tidy3 = nn.Conv2d(128, 9, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        d1 = self.ReLU(self.down1(x))\n",
    "        d2 = self.ReLU(self.down2(d1))\n",
    "        d3 = self.ReLU(self.down3(d2))\n",
    "        t1 = self.tidy1(d1).view(batch_size, -1)\n",
    "        t2 = self.tidy2(d2).view(batch_size, -1)\n",
    "        t3 = self.tidy3(d3).view(batch_size, -1)\n",
    "        return torch.cat((t1, t2, t3), dim=1)\n",
    "\n",
    "\n",
    "class attention_net(nn.Module):\n",
    "    def __init__(self, topN=4):\n",
    "        super(attention_net, self).__init__()\n",
    "        self.pretrained_model = resnet50(pretrained=True)\n",
    "        self.pretrained_model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.pretrained_model.fc = nn.Linear(512 * 4, 105)#105개의 class\n",
    "        self.proposal_net = ProposalNet()\n",
    "        self.topN = topN\n",
    "        self.concat_net = nn.Linear(2048 * (CAT_NUM + 1), 105) \n",
    "        self.partcls_net = nn.Linear(512 * 4, 105) \n",
    "        _, edge_anchors, _ = generate_default_anchor_maps()\n",
    "        self.pad_side = 224\n",
    "        self.edge_anchors = (edge_anchors + 224).astype(np.int)\n",
    "\n",
    "    def forward(self, x):\n",
    "        resnet_out, rpn_feature, feature = self.pretrained_model(x)\n",
    "        x_pad = F.pad(x, (self.pad_side, self.pad_side, self.pad_side, self.pad_side), mode='constant', value=0)\n",
    "        batch = x.size(0)\n",
    "        # we will reshape rpn to shape: batch * nb_anchor\n",
    "        rpn_score = self.proposal_net(rpn_feature.detach())\n",
    "        all_cdds = [\n",
    "            np.concatenate((x.reshape(-1, 1), self.edge_anchors.copy(), np.arange(0, len(x)).reshape(-1, 1)), axis=1)\n",
    "            for x in rpn_score.data.cpu().numpy()]\n",
    "        top_n_cdds = [hard_nms(x, topn=self.topN, iou_thresh=0.25) for x in all_cdds]\n",
    "        top_n_cdds = np.array(top_n_cdds)\n",
    "        top_n_index = top_n_cdds[:, :, -1].astype(np.int)\n",
    "        top_n_index = torch.from_numpy(top_n_index).cuda()\n",
    "        top_n_prob = torch.gather(rpn_score, dim=1, index=top_n_index)\n",
    "        part_imgs = torch.zeros([batch, self.topN, 3, 224, 224]).cuda()\n",
    "        for i in range(batch):\n",
    "            for j in range(self.topN):\n",
    "                [y0, x0, y1, x1] = top_n_cdds[i][j, 1:5].astype(np.int)\n",
    "                part_imgs[i:i + 1, j] = F.interpolate(x_pad[i:i + 1, :, y0:y1, x0:x1], size=(224, 224), mode='bilinear',\n",
    "                                                      align_corners=True)\n",
    "        part_imgs = part_imgs.view(batch * self.topN, 3, 224, 224)\n",
    "        _, _, part_features = self.pretrained_model(part_imgs.detach())\n",
    "        part_feature = part_features.view(batch, self.topN, -1)\n",
    "        part_feature = part_feature[:, :CAT_NUM, ...].contiguous()\n",
    "        part_feature = part_feature.view(batch, -1)\n",
    "        concat_out = torch.cat([part_feature, feature], dim=1)\n",
    "        concat_logits = self.concat_net(concat_out)\n",
    "        raw_logits = resnet_out\n",
    "        part_logits = self.partcls_net(part_features).view(batch, self.topN, -1)\n",
    "        return [raw_logits, concat_logits, part_logits, top_n_index, top_n_prob]\n",
    "\n",
    "\n",
    "def list_loss(logits, targets):\n",
    "    temp = F.log_softmax(logits, -1)\n",
    "    loss = [-temp[i][targets[i].item()] for i in range(logits.size(0))]\n",
    "    return torch.stack(loss)\n",
    "\n",
    "\n",
    "def ranking_loss(score, targets, proposal_num=PROPOSAL_NUM):\n",
    "    loss = Variable(torch.zeros(1).cuda())\n",
    "    batch_size = score.size(0)\n",
    "    for i in range(proposal_num):\n",
    "        targets_p = (targets > targets[:, i].unsqueeze(1)).type(torch.cuda.FloatTensor)\n",
    "        pivot = score[:, i].unsqueeze(1)\n",
    "        loss_p = (1 - pivot + score) * targets_p\n",
    "        loss_p = torch.sum(F.relu(loss_p))\n",
    "        loss += loss_p\n",
    "    return loss / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5dcf03",
   "metadata": {
    "id": "4d5dcf03"
   },
   "source": [
    "# train 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bW1LYZr8h7HT",
   "metadata": {
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1630037134529,
     "user": {
      "displayName": "Gyeom",
      "photoUrl": "",
      "userId": "17196245438344588530"
     },
     "user_tz": -540
    },
    "id": "bW1LYZr8h7HT"
   },
   "outputs": [],
   "source": [
    "train_losses=[]\n",
    "train_accues=[]\n",
    "val_losses=[]\n",
    "val_accues=[]\n",
    "test_losses=[]\n",
    "test_accues=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d79be495",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "df4f0b82f9f94217a3753f4b6b0f35b8",
      "b6aa55dd810944d08ff453de4acbef31",
      "dad076dce28d4f66af4ee10b53dcbc8e",
      "fbce98141dc344238c413e754a62c298",
      "cc03b127d34347068fafb714227a004b",
      "6c504a28c2534dc49e41d65dde6c784f",
      "0e62a51eb50c4d899da7fbed94ef7296",
      "c311ec063eb447f0be2c5dafa8bb28c0",
      "52736553a8fb4b299f8fe8ec41c515c7",
      "030a6e606e7f49bfac46c536ee1a4089",
      "f29514ff0ec14143a7ef8b15c4043d46"
     ]
    },
    "executionInfo": {
     "elapsed": 12839,
     "status": "ok",
     "timestamp": 1630037148788,
     "user": {
      "displayName": "Gyeom",
      "photoUrl": "",
      "userId": "17196245438344588530"
     },
     "user_tz": -540
    },
    "id": "d79be495",
    "outputId": "d61c946b-8a96-4a05-a1ee-c90f860234e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4f0b82f9f94217a3753f4b6b0f35b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "import os\n",
    "import torch.utils.data\n",
    "\n",
    "start_epoch = 1\n",
    "save_dir = os.path.join(save_dir, datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "if os.path.exists(save_dir):\n",
    "    raise NameError('model dir exists!')\n",
    "os.makedirs(save_dir)\n",
    "logging = init_log(save_dir)\n",
    "_print = logging.info\n",
    "\n",
    "\n",
    "\n",
    "net = attention_net(topN=PROPOSAL_NUM)\n",
    "if resume:\n",
    "    ckpt = torch.load(resume)\n",
    "    net.load_state_dict(ckpt['net_state_dict'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "creterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# define optimizers\n",
    "raw_parameters = list(net.pretrained_model.parameters())\n",
    "part_parameters = list(net.proposal_net.parameters())\n",
    "concat_parameters = list(net.concat_net.parameters())\n",
    "partcls_parameters = list(net.partcls_net.parameters())\n",
    "\n",
    "raw_optimizer = torch.optim.SGD(raw_parameters, lr=LR, momentum=0.9, weight_decay=WD)\n",
    "concat_optimizer = torch.optim.SGD(concat_parameters, lr=LR, momentum=0.9, weight_decay=WD)\n",
    "part_optimizer = torch.optim.SGD(part_parameters, lr=LR, momentum=0.9, weight_decay=WD)\n",
    "partcls_optimizer = torch.optim.SGD(partcls_parameters, lr=LR, momentum=0.9, weight_decay=WD)\n",
    "schedulers = [MultiStepLR(raw_optimizer, milestones=[60, 100], gamma=0.1),\n",
    "              MultiStepLR(concat_optimizer, milestones=[60, 100], gamma=0.1),\n",
    "              MultiStepLR(part_optimizer, milestones=[60, 100], gamma=0.1),\n",
    "              MultiStepLR(partcls_optimizer, milestones=[60, 100], gamma=0.1)]\n",
    "net = net.cuda()#원래 net.cuda()\n",
    "net = DataParallel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bd9a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb2bd9a9",
    "outputId": "107fdde4-e90e-4c98-a0a8-c49b11adbcfb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# 이어짐\n",
    "\n",
    "for epoch in range(start_epoch, 11):\n",
    "    for scheduler in schedulers:\n",
    "        scheduler.step()\n",
    "\n",
    "    # begin training\n",
    "    _print('--' * 50)\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        img, label = data[0].cuda(), data[1].cuda()\n",
    "        batch_size = img.size(0)\n",
    "        raw_optimizer.zero_grad()\n",
    "        part_optimizer.zero_grad()\n",
    "        concat_optimizer.zero_grad()\n",
    "        partcls_optimizer.zero_grad()\n",
    "\n",
    "        raw_logits, concat_logits, part_logits, _, top_n_prob = net(img)                 \n",
    "        part_loss = list_loss(part_logits.view(batch_size * PROPOSAL_NUM, -1),\n",
    "                                    label.unsqueeze(1).repeat(1, PROPOSAL_NUM).view(-1)).view(batch_size, PROPOSAL_NUM)\n",
    "        raw_loss = creterion(raw_logits, label)\n",
    "        concat_loss = creterion(concat_logits, label)\n",
    "        rank_loss = ranking_loss(top_n_prob, part_loss)\n",
    "        partcls_loss = creterion(part_logits.view(batch_size * PROPOSAL_NUM, -1),\n",
    "                                 label.unsqueeze(1).repeat(1, PROPOSAL_NUM).view(-1))\n",
    "\n",
    "        total_loss = raw_loss + rank_loss + concat_loss + partcls_loss\n",
    "        total_loss.backward()\n",
    "        raw_optimizer.step()\n",
    "        part_optimizer.step()\n",
    "        concat_optimizer.step()\n",
    "        partcls_optimizer.step()\n",
    "        progress_bar(i, len(train_dataloader ), 'train')\n",
    "\n",
    "    if epoch % SAVE_FREQ == 0:\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        total = 0\n",
    "        net.eval()\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            with torch.no_grad():\n",
    "                img, label = data[0].cuda(), data[1].cuda()\n",
    "                batch_size = img.size(0)\n",
    "                _, concat_logits, _, _, _ = net(img)\n",
    "                # calculate loss\n",
    "                concat_loss = creterion(concat_logits, label)\n",
    "                # calculate accuracy\n",
    "                _, concat_predict = torch.max(concat_logits, 1)\n",
    "                total += batch_size\n",
    "                train_correct += torch.sum(concat_predict.data == label.data)\n",
    "                train_loss += concat_loss.item() * batch_size\n",
    "                progress_bar(i, len(train_dataloader), 'eval train set')\n",
    "\n",
    "        train_acc = float(train_correct) / total\n",
    "        train_loss = train_loss / total\n",
    "\n",
    "        _print(\n",
    "            'epoch:{} - train loss: {:.3f} and train acc: {:.3f} total sample: {}'.format(\n",
    "                epoch,\n",
    "                train_loss,\n",
    "                train_acc,\n",
    "                total))\n",
    "        \n",
    "        # Validaiton 해보기\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(val_dataloader):\n",
    "            with torch.no_grad():\n",
    "                img, label = data[0].cuda(), data[1].cuda()#data[0].cuda(), data[1].cuda()\n",
    "                batch_size = img.size(0)\n",
    "                _, concat_logits, _, _, _ = net(img)\n",
    "                # calculate loss\n",
    "                concat_loss = creterion(concat_logits, label)\n",
    "                # calculate accuracy\n",
    "                _, concat_predict = torch.max(concat_logits, 1)\n",
    "                total += batch_size\n",
    "                val_correct += torch.sum(concat_predict.data == label.data)\n",
    "                val_loss += concat_loss.item() * batch_size\n",
    "                progress_bar(i, len(val_dataloader), 'eval validation set')\n",
    "\n",
    "        val_acc = float(val_correct) / total\n",
    "        val_loss = val_loss / total\n",
    "        _print(\n",
    "            'epoch:{} - validation loss: {:.3f} and validation acc: {:.3f} total sample: {}'.format(\n",
    "                epoch,\n",
    "                val_loss,\n",
    "                val_acc,\n",
    "                total))\n",
    "\n",
    "        # evaluate on test set\n",
    "        test_loss = 0\n",
    "        test_correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(test_dataloader):\n",
    "            with torch.no_grad():\n",
    "                img, label = data[0].cuda(), data[1].cuda()#data[0].cuda(), data[1].cuda()\n",
    "                batch_size = img.size(0)\n",
    "                _, concat_logits, _, _, _ = net(img)\n",
    "                # calculate loss\n",
    "                concat_loss = creterion(concat_logits, label)\n",
    "                # calculate accuracy\n",
    "                _, concat_predict = torch.max(concat_logits, 1)\n",
    "                total += batch_size\n",
    "                test_correct += torch.sum(concat_predict.data == label.data)\n",
    "                test_loss += concat_loss.item() * batch_size\n",
    "                progress_bar(i, len(test_dataloader), 'eval test set')\n",
    "\n",
    "        test_acc = float(test_correct) / total\n",
    "        test_loss = test_loss / total\n",
    "        _print(\n",
    "            'epoch:{} - test loss: {:.3f} and test acc: {:.3f} total sample: {}'.format(\n",
    "                epoch,\n",
    "                test_loss,\n",
    "                test_acc,\n",
    "                total))\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accues.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accues.append(val_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accues.append(test_acc)        \n",
    "\n",
    "        # save model\n",
    "        net_state_dict = net.module.state_dict()\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'net_state_dict': net_state_dict},\n",
    "            os.path.join(save_dir, '%03d.ckpt' % epoch)) \n",
    "\n",
    "print('finishing training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c66e63",
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "aborted",
     "timestamp": 1630036865072,
     "user": {
      "displayName": "Gyeom",
      "photoUrl": "",
      "userId": "17196245438344588530"
     },
     "user_tz": -540
    },
    "id": "sFoCAYyLR_DI"
   },
   "source": [
    "# 그래프 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23tsEy_ueCa",
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "aborted",
     "timestamp": 1630036865073,
     "user": {
      "displayName": "Gyeom",
      "photoUrl": "",
      "userId": "17196245438344588530"
     },
     "user_tz": -540
    },
    "id": "e23tsEy_ueCa"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_accues,'-o')\n",
    "plt.plot(val_accues,'-o')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['Train','Validation'])\n",
    "plt.title('Train vs Validation Accuracy')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Cy5lzw0vu5o9",
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "aborted",
     "timestamp": 1630036865075,
     "user": {
      "displayName": "Gyeom",
      "photoUrl": "",
      "userId": "17196245438344588530"
     },
     "user_tz": -540
    },
    "id": "Cy5lzw0vu5o9"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_accues,'-o')\n",
    "plt.plot(test_accues,'-o')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['Train','Test'])\n",
    "plt.title('Train vs Test Loss')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6bae0b",
   "metadata": {},
   "source": [
    "## 이후 최종 모델을 위해 Train, Valid, Test dataset을 합쳐서 train시켰습니다."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NTS_Net_ver_0.2.ipynb",
   "provenance": [
    {
     "file_id": "1Tum8PP8NE-vy6hGucwcTVQ2t12U4VCHB",
     "timestamp": 1628817666401
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "030a6e606e7f49bfac46c536ee1a4089": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0e62a51eb50c4d899da7fbed94ef7296": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52736553a8fb4b299f8fe8ec41c515c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c504a28c2534dc49e41d65dde6c784f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b6aa55dd810944d08ff453de4acbef31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c311ec063eb447f0be2c5dafa8bb28c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cc03b127d34347068fafb714227a004b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f29514ff0ec14143a7ef8b15c4043d46",
      "placeholder": "​",
      "style": "IPY_MODEL_030a6e606e7f49bfac46c536ee1a4089",
      "value": " 97.8M/97.8M [00:01&lt;00:00, 70.1MB/s]"
     }
    },
    "dad076dce28d4f66af4ee10b53dcbc8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e62a51eb50c4d899da7fbed94ef7296",
      "placeholder": "​",
      "style": "IPY_MODEL_6c504a28c2534dc49e41d65dde6c784f",
      "value": "100%"
     }
    },
    "df4f0b82f9f94217a3753f4b6b0f35b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dad076dce28d4f66af4ee10b53dcbc8e",
       "IPY_MODEL_fbce98141dc344238c413e754a62c298",
       "IPY_MODEL_cc03b127d34347068fafb714227a004b"
      ],
      "layout": "IPY_MODEL_b6aa55dd810944d08ff453de4acbef31"
     }
    },
    "f29514ff0ec14143a7ef8b15c4043d46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fbce98141dc344238c413e754a62c298": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52736553a8fb4b299f8fe8ec41c515c7",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c311ec063eb447f0be2c5dafa8bb28c0",
      "value": 102502400
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
